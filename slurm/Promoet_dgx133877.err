
The following have been reloaded with a version change:
  1) gcc/13.1.0 => gcc/13.2.0

wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: ejxie (erik_xie) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in ./wandb/run-20250311_214414-kede4p67
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sandy-fire-56
wandb: ‚≠êÔ∏è View project at https://wandb.ai/erik_xie/poet
wandb: üöÄ View run at https://wandb.ai/erik_xie/poet/runs/kede4p67
/home/jix836/.conda/envs/promoet/lib/python3.9/site-packages/lightning/fabric/connector.py:554: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!
  rank_zero_warn(
/home/jix836/.conda/envs/promoet/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:168: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python scripts/train.py ...
  rank_zero_warn(
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/jix836/.conda/envs/promoet/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:168: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python scripts/train.py ...
  rank_zero_warn(
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name  | Type | Params
-------------------------------
0 | model | PoET | 201 M 
-------------------------------
201 M     Trainable params
0         Non-trainable params
201 M     Total params
806.150   Total estimated model params size (MB)
/home/jix836/.conda/envs/promoet/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 112 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/n/lw_groups/hms/sysbio/marks/lab/jix836/Promoter_Poet_private/poet/models/modules/attention_flash.py:213: UserWarning: Flash attention requires float16 or bfloat16 not torch.float32. Converting to bfloat16.
  warnings.warn(
/n/lw_groups/hms/sysbio/marks/lab/jix836/Promoter_Poet_private/poet/models/modules/attention_flash.py:213: UserWarning: Flash attention requires float16 or bfloat16 not torch.float32. Converting to bfloat16.
  warnings.warn(
/home/jix836/.conda/envs/promoet/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('validation_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/jix836/.conda/envs/promoet/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('validation_perplexity', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/jix836/.conda/envs/promoet/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 112 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
`Trainer.fit` stopped: `max_epochs=6` reached.
